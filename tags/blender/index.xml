<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Blender - 标签 - pigLoveRabbit的网站</title><link>https://example.com/tags/blender/</link><description>Blender - 标签 - pigLoveRabbit的网站</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 01 Dec 2024 20:00:00 +0000</lastBuildDate><atom:link href="https://example.com/tags/blender/" rel="self" type="application/rss+xml"/><item><title>Attention机制学习</title><link>https://example.com/llm_learning/</link><pubDate>Sun, 01 Dec 2024 20:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://example.com/llm_learning/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="q-k-v">Q K V</h2>
<p>在深度学习中，很多 LLM 的训练都使用 Transformer 架构，而在 Transformer 架构中计算的过程涉及到的最关键的就是注意力，它是整个过程中重要的基础。注意力抽象出了 3 个重要的概念，在计算过程中对应着 3 个矩阵，如下所示：</p>]]></description></item><item><title>Blender常用操作</title><link>https://example.com/blender%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</link><pubDate>Thu, 20 Jul 2023 19:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://example.com/blender%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</guid><description><![CDATA[<p></p>
<!-- more -->
<h5 id="查看顶点面法向量">查看顶点/面法向量</h5>
<p>左上角下拉框选择<code>编辑模式</code>，在右边3个Tab中选择<code>面模式</code>
<br>
然后点overlays下拉箭头，就能看到normal的选项了<br>
<br>
第一个Tab就是顶点法向，第三个Tab就是面法向。</p>]]></description></item></channel></rss>